{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ast\n",
    "import urllib\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import scipy.optimize\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseDataFromFile(fname):\n",
    "    \"\"\" Read through the file and return results\"\"\"\n",
    "    for l in open(fname, encoding=\"utf8\"):\n",
    "        yield ast.literal_eval(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hours': 0.3, 'gameID': 'b96045472', 'hours_transformed': 0.37851162325372983, 'early_access': False, 'date': '2015-04-08', 'text': '+1', 'userID': 'u01561183'}\n"
     ]
    }
   ],
   "source": [
    "dataset = list(parseDataFromFile(\"C:\\\\Users\\\\ramasarma\\\\Documents\\\\UCSD\\\\Fall 2020\\\\CSE 258\\\\Assignment1\\\\assignment1\\\\train.json\"))\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175000\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))\n",
    "training_validation_boundary = 165000\n",
    "# Here, we compute the boundaries between the training and validation set\n",
    "training_set = dataset[:training_validation_boundary]\n",
    "validation_set = dataset[training_validation_boundary:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done...\n"
     ]
    }
   ],
   "source": [
    "usersPerGame = defaultdict(set) # list of userIDs per game\n",
    "gamesPerUser = defaultdict(set) # list of gameIDs per user\n",
    "reviewsPerUser = defaultdict(list)\n",
    "reviewsPerGame = defaultdict(list)\n",
    "gameIDs = [] # This contains the entire list of games\n",
    "\n",
    "usersPerGame_training = defaultdict(set) # list of userIDs per game\n",
    "gamesPerUser_training = defaultdict(set) # list of gameIDs per user\n",
    "reviewsPerUser_training = defaultdict(list)\n",
    "reviewsPerGame_training = defaultdict(list)\n",
    "gameIDs_training = [] # This contains the entire list of games\n",
    "\n",
    "usersPerGame_validation = defaultdict(set) # list of userIDs per game\n",
    "gamesPerUser_validation = defaultdict(set) # list of gameIDs per user\n",
    "reviewsPerUser_validation = defaultdict(list)\n",
    "reviewsPerGame_validation = defaultdict(list)\n",
    "gameIDs_validation = [] # This contains the entire list of games\n",
    "\n",
    "# Computing maps and lists for all samples\n",
    "for d in dataset:\n",
    "    userID, gameID = d['userID'], d['gameID']\n",
    "    usersPerGame[gameID].add(userID)\n",
    "    gamesPerUser[userID].add(gameID)\n",
    "    reviewsPerUser[userID].append(d)\n",
    "    reviewsPerGame[gameID].append(d)\n",
    "    if gameID not in gameIDs:\n",
    "        gameIDs.append(gameID)\n",
    "        \n",
    "# Computing maps and lists for training set samples\n",
    "for d in training_set:\n",
    "    userID, gameID = d['userID'], d['gameID']\n",
    "    usersPerGame_training[gameID].add(userID)\n",
    "    gamesPerUser_training[userID].add(gameID) # list of gameIDs per user\n",
    "    reviewsPerUser_training[userID].append(d)\n",
    "    reviewsPerGame_training[gameID].append(d)\n",
    "    if gameID not in gameIDs_training:\n",
    "        gameIDs_training.append(gameID)\n",
    "        \n",
    "# Computing maps and lists for training set samples\n",
    "for d in validation_set:\n",
    "    userID, gameID = d['userID'], d['gameID']\n",
    "    usersPerGame_validation[gameID].add(userID)\n",
    "    gamesPerUser_validation[userID].add(gameID) # list of gameIDs per user\n",
    "    reviewsPerUser_validation[userID].append(d)\n",
    "    reviewsPerGame_validation[gameID].append(d)\n",
    "    if gameID not in gameIDs_training:\n",
    "        gameIDs_validation.append(gameID)\n",
    "print(\"Done...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in training_set:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_duplication(dataset, start, end):\n",
    "    assert(start <= len(dataset) and end <= len(dataset))\n",
    "    new_set = []\n",
    "    print(\"Start = {}, end = {}\".format(start, end))\n",
    "    for d in dataset[start:end]:\n",
    "        user, game = d['userID'], d['gameID']\n",
    "        gamesNotPlayed = []\n",
    "        for game in gameIDs:\n",
    "            if(game not in gamesPerUser[user]):\n",
    "                gamesNotPlayed.append(game)\n",
    "                \n",
    "        random.seed()\n",
    "        randomGame = random.choice(gamesNotPlayed)\n",
    "        new_data = {'hours': 0, 'gameID': str(randomGame),\\\n",
    "                'hours_transformed': 0, 'early_access': False,\\\n",
    "                'date': '', 'text': '', 'userID': str(userID)}\n",
    "        new_set.append(new_data)\n",
    "    return dataset + new_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start = 0, end = 165000\n",
      "Start = 0, end = 10000\n",
      "Lengths of new datasets = 330000 and 20000\n"
     ]
    }
   ],
   "source": [
    "new_training_set = perform_duplication(training_set, 0, len(training_set))\n",
    "new_validation_set = perform_duplication(validation_set, 0, len(validation_set))\n",
    "print(\"Lengths of new datasets = {} and {}\".format(len(new_training_set), len(new_validation_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def jaccard(s1, s2):\n",
    "    numerator = len(s1.intersection(s2))\n",
    "    denominator = len(s1.union(s2))\n",
    "    return numerator/denominator\n",
    "\n",
    "def cosine(s1, s2):\n",
    "    numerator = len(s1.intersection(s2))\n",
    "    denominator = math.sqrt(len(s1) * len(s2))\n",
    "    return numerator/denominator\n",
    "\n",
    "def similarity_popularity(dataset, sim_th, pop_th, start, end):\n",
    "    y_actual, y_predicted = [], []\n",
    "    idx, numerator = 0, 0\n",
    "    for d in dataset:\n",
    "        user, game = d['userID'], d['gameID']\n",
    "        # Compute game to game similarity\n",
    "        Users_g = usersPerGame[game]\n",
    "        maxSim = -1\n",
    "        for g2 in gamesPerUser[user]:\n",
    "            if(g2 == game):\n",
    "                continue\n",
    "            Users_g2 = usersPerGame[g2]\n",
    "            sim = jaccard(Users_g, Users_g2)\n",
    "            maxSim = max(sim, maxSim)\n",
    "    \n",
    "        if(maxSim > sim_th or len(reviewsPerGame[game]) > pop_th):\n",
    "            y_predicted.append(1)\n",
    "        else:\n",
    "            y_predicted.append(0)\n",
    "    \n",
    "        if(idx >= start and idx < end):\n",
    "            y_actual.append(0)\n",
    "        else:\n",
    "            y_actual.append(1)\n",
    "        \n",
    "        if(y_actual[idx] == y_predicted[idx]):\n",
    "            numerator += 1\n",
    "            \n",
    "        idx += 1\n",
    "    # Compute the ratio of numerator / denominator\n",
    "    return numerator/len(y_predicted)\n",
    "\n",
    "def similarity_popularity_cosine(dataset, sim_th, pop_th, start, end):\n",
    "    y_actual, y_predicted = [], []\n",
    "    idx, numerator = 0, 0\n",
    "    for d in dataset:\n",
    "        user, game = d['userID'], d['gameID']\n",
    "        # Compute game to game similarity\n",
    "        Users_g = usersPerGame[game]\n",
    "        maxSim = -1\n",
    "        for g2 in gamesPerUser[user]:\n",
    "            if(g2 == game):\n",
    "                continue\n",
    "            Users_g2 = usersPerGame[g2]\n",
    "            sim = cosine(Users_g, Users_g2)\n",
    "            maxSim = max(sim, maxSim)\n",
    "        # \n",
    "        if(maxSim > sim_th or len(reviewsPerGame[game]) > pop_th):\n",
    "            y_predicted.append(1)\n",
    "        else:\n",
    "            y_predicted.append(0)\n",
    "    \n",
    "        if(idx >= start and idx < end):\n",
    "            y_actual.append(0)\n",
    "        else:\n",
    "            y_actual.append(1)\n",
    "        \n",
    "        if(y_actual[idx] == y_predicted[idx]):\n",
    "            numerator += 1\n",
    "            \n",
    "        idx += 1\n",
    "    # Compute the ratio of numerator / denominator\n",
    "    return numerator/len(y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sim_thresholds = np.arange(0.1, 0.25, 0.01)\n",
    "# pop_thresholds = np.arange(20, 120, 20)\n",
    "# accuracies = defaultdict(dict)\n",
    "# for sim_th in sim_thresholds:\n",
    "#     for pop_th in pop_thresholds:\n",
    "#         acc = similarity_popularity(new_validation_set, sim_th, pop_th,\n",
    "#                                     len(new_validation_set)//2,len(new_validation_set))\n",
    "#         accuracies[sim_th][pop_th] = acc\n",
    "#         print(\"Accuracy at (x, y) = ({}, {}) is {}\".format(sim_th, pop_th, acc))\n",
    "\n",
    "# accuracy_values = []\n",
    "# for key, value in accuracies.items():\n",
    "#     values = []\n",
    "#     for k2, v2 in value.items():\n",
    "#         values.append(v2)\n",
    "#     accuracy_values.append(values)\n",
    "# print(sim_thresholds)\n",
    "# print(pop_thresholds)\n",
    "# print(accuracy_values)\n",
    "# fig = plt.figure()\n",
    "# ax = plt.axes(projection='3d')\n",
    "# ax.contour3D(pop_thresholds, sim_thresholds, accuracy_values, 50, cmap='binary')\n",
    "# ax.set_xlabel('x')\n",
    "# ax.set_ylabel('y')\n",
    "# ax.set_zlabel('z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = []\n",
    "prediction_file = open(\"C:\\\\Users\\\\ramasarma\\\\Documents\\\\UCSD\\\\Fall 2020\\\\CSE 258\\\\Assignment1\\\\assignment1\\\\predictions_Played_60.txt\", 'w')\n",
    "for l in open(\"C:\\\\Users\\\\ramasarma\\\\Documents\\\\UCSD\\\\Fall 2020\\\\CSE 258\\\\Assignment1\\\\assignment1\\\\pairs_Played.txt\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "        #header\n",
    "        prediction_file.write(l)\n",
    "        continue\n",
    "    u,g = l.strip().split('-')\n",
    "    val = {}\n",
    "    val['userID'], val['gameID'] = u, g\n",
    "    test_set.append(val)\n",
    "\n",
    "\n",
    "def similarity_popularity(dataset, sim_th, pop_th):\n",
    "    idx, numerator = 0, 0\n",
    "    for d in dataset:\n",
    "        user, game = d['userID'], d['gameID']\n",
    "        # Compute game to game similarity\n",
    "        Users_g = usersPerGame[game]\n",
    "        maxSim = -1\n",
    "        for g2 in gamesPerUser[user]:\n",
    "            if(g2 == game):\n",
    "                continue\n",
    "            Users_g2 = usersPerGame[g2]\n",
    "            sim = jaccard(Users_g, Users_g2)\n",
    "            maxSim = max(sim, maxSim)\n",
    "    \n",
    "        if(maxSim > sim_th or len(reviewsPerGame[game]) > pop_th):\n",
    "            prediction_file.write(user + '-' + game + \",1\\n\")\n",
    "        else:\n",
    "            prediction_file.write(user + '-' + game + \",0\\n\")\n",
    "            \n",
    "similarity_popularity(test_set, 0.11, 60)\n",
    "prediction_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_popularity_features(dataset):\n",
    "    idx, numerator = 0, 0\n",
    "    features = []\n",
    "#     max_length = -1\n",
    "#     for game in reviewsPerGame:\n",
    "#         max_length = max(max_length, len(reviewsPerGame[game]))\n",
    "#     print(\"Max length of all games is {}\".format(max_length))\n",
    "    for d in dataset:\n",
    "        u, g = d['userID'], d['gameID']\n",
    "        games_u = gamesPerUser[u]\n",
    "        max_sim = -1\n",
    "        for u2 in usersPerGame[g]:\n",
    "            if u2 == u:\n",
    "                continue\n",
    "            games_u2 = gamesPerUser[u2]\n",
    "            sim = jaccard(games_u, games_u2)\n",
    "            max_sim = max(max_sim, sim)\n",
    "        # Computing updated features  Similarity between users and len(reviewsPerGame)\n",
    "        features.append([1, max_sim, len(reviewsPerUser[u])])\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_confusion_matrix(Y_actual, Y_predict):\n",
    "    \"\"\" This function is used to compute the confusion matrix\"\"\"\n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    assert(len(Y_actual) == len(Y_predict))\n",
    "    for i in range(len(Y_actual)):\n",
    "        if(Y_actual[i] == 1 and Y_predict[i] == 1):\n",
    "            TP += 1\n",
    "        elif(Y_actual[i] == 0 and Y_predict[i] == 1):\n",
    "            FP += 1\n",
    "        elif(Y_actual[i] == 0 and Y_predict[i] == 0):\n",
    "            TN += 1\n",
    "        elif(Y_actual[i] == 1 and Y_predict[i] == 0):\n",
    "            FN += 1\n",
    "    return (TP, TN, FP, FN)\n",
    "\n",
    "# Compute the ratios for computing the TP, TN, FP and FN\n",
    "def compute_rates(TP, TN, FP, FN):\n",
    "    TPR = TP / (TP + FN)\n",
    "    TNR = TN / (TN + FP)\n",
    "    FPR = FP / (FP + TN)\n",
    "    FNR = FN / (FN + TP)\n",
    "    BER = 0.5 * (FPR + FNR)\n",
    "    return (TPR, TNR, FPR, FNR, BER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy @ lamb = 1e-05 is 0.68575\n",
      "BER @lamb = 1e-05 is 0.31425\n",
      "Accuracy @ lamb = 0.0001 is 0.6855\n",
      "BER @lamb = 0.0001 is 0.3145\n",
      "Accuracy @ lamb = 0.001 is 0.70075\n",
      "BER @lamb = 0.001 is 0.29925\n",
      "Accuracy @ lamb = 0.01 is 0.8205\n",
      "BER @lamb = 0.01 is 0.1795\n",
      "Accuracy @ lamb = 0.1 is 0.83625\n",
      "BER @lamb = 0.1 is 0.16375\n",
      "Accuracy @ lamb = 1.0 is 0.84265\n",
      "BER @lamb = 1.0 is 0.15735\n",
      "Accuracy @ lamb = 10.0 is 0.8442\n",
      "BER @lamb = 10.0 is 0.1558\n",
      "Accuracy @ lamb = 100.0 is 0.8442\n",
      "BER @lamb = 100.0 is 0.1558\n",
      "Accuracy @ lamb = 1000.0 is 0.84425\n",
      "BER @lamb = 1000.0 is 0.15575\n",
      "Accuracy @ lamb = 10000.0 is 0.84425\n",
      "BER @lamb = 10000.0 is 0.15575\n",
      "Accuracies = {1e-05: 0.68575, 0.0001: 0.6855, 0.001: 0.70075, 0.01: 0.8205, 0.1: 0.83625, 1.0: 0.84265, 10.0: 0.8442, 100.0: 0.8442, 1000.0: 0.84425, 10000.0: 0.84425}\n"
     ]
    }
   ],
   "source": [
    "X_train = similarity_popularity_features(new_training_set)\n",
    "Y_train = [1 if i < len(new_training_set)/2 else 0 for i in range(len(new_training_set))]\n",
    "\n",
    "X_test = similarity_popularity_features(new_validation_set)\n",
    "Y_test = [1 if i < len(new_validation_set)/2 else 0 for i in range(len(new_validation_set))]\n",
    "\n",
    "lambdas = [10 ** i for i in np.arange(-5,5,dtype=float)]\n",
    "accuracies = {}\n",
    "for lamb in lambdas:\n",
    "    model = linear_model.LogisticRegression(C=lamb,class_weight='balanced')\n",
    "    model.fit(X_train, Y_train)\n",
    "    Y_predictions = model.predict(X_test)\n",
    "\n",
    "    (TP, TN, FP, FN) = compute_confusion_matrix(Y_test, Y_predictions)\n",
    "    (TPR, TNR, FPR, FNR, BER) = compute_rates(TP, TN, FP, FN)\n",
    "    acc = (TP + TN)/(TP + TN + FP + FN)\n",
    "    print(\"Accuracy @ lamb = {} is {}\".format(lamb, acc))\n",
    "    print(\"BER @lamb = {} is {}\".format(lamb, BER))\n",
    "    accuracies[lamb] = acc\n",
    "print(\"Accuracies = {}\".format(accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done writing to the file!\n"
     ]
    }
   ],
   "source": [
    "test_set = []\n",
    "prediction_file = open(\"C:\\\\Users\\\\ramasarma\\\\Documents\\\\UCSD\\\\Fall 2020\\\\CSE 258\\\\Assignment1\\\\assignment1\\\\predictions_Played_classifierv3.txt\", 'w')\n",
    "for l in open(\"C:\\\\Users\\\\ramasarma\\\\Documents\\\\UCSD\\\\Fall 2020\\\\CSE 258\\\\Assignment1\\\\assignment1\\\\pairs_Played.txt\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "        #header\n",
    "        prediction_file.write(l)\n",
    "        continue\n",
    "    u,g = l.strip().split('-')\n",
    "    val = {}\n",
    "    val['userID'], val['gameID'] = u, g\n",
    "    test_set.append(val)\n",
    "\n",
    "model = linear_model.LogisticRegression(C=10000,class_weight='balanced')\n",
    "model.fit(X_train, Y_train)\n",
    "X_val = similarity_popularity_features(test_set)\n",
    "Y_val = model.predict(X_val)\n",
    "for i in range(len(Y_val)):\n",
    "    u, g = test_set[i]['userID'], test_set[i]['gameID']\n",
    "    pred = Y_val[i]\n",
    "    prediction_file.write(u + \"-\" + g + \",\" + str(pred) + \"\\n\")\n",
    "\n",
    "prediction_file.close()\n",
    "print(\"Done writing to the file!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

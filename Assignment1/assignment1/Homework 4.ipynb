{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Libraries\n",
    "import numpy as np\n",
    "import ast\n",
    "import urllib\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import scipy.optimize\n",
    "from sklearn import linear_model\n",
    "import math\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import gzip\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseDataFromFile(fname):\n",
    "    \"\"\" Read and convert the input to a list of dicts\"\"\"\n",
    "    for l in open(fname, encoding=\"utf-8\"):\n",
    "        yield ast.literal_eval(l)\n",
    "        \n",
    "def readGz(path):\n",
    "    for l in gzip.open(path, 'rt', encoding=\"utf-8\"):\n",
    "        yield eval(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175000\n"
     ]
    }
   ],
   "source": [
    "fname = \"C:\\\\Users\\\\ramasarma\\\\Documents\\\\UCSD\\\\Fall 2020\\\\CSE 258\\\\Assignment1\\\\assignment1\\\\train_Category.json\"\n",
    "data = list(parseDataFromFile(fname))\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'userID': 'u74382925', 'genre': 'Adventure', 'early_access': False, 'reviewID': 'r75487422', 'hours': 4.1, 'text': 'Short Review:\\nA good starting chapter for this series, despite the main character being annoying (for now) and a short length. The story is good and actually gets more interesting. Worth the try.\\nLong Review:\\nBlackwell Legacy is the first on the series of (supposedly) 5 games that talks about the main protagonist, Rosangela Blackwell, as being a so called Medium, and in this first chapter we get to know how her story will start and how she will meet her adventure companion Joey...and really, that\\'s really all for for now and that\\'s not a bad thing, because in a way this game wants to show how hard her new job is, and that she cannot escape her destiny as a medium.\\nMy biggest complain for this chapter, except the short length, it\\'s the main protagonist being a \"bit\" too annoying to be likeable, and most of her dialogues will always be about complaining or just be annoyed. Understandable, sure, but lighten\\' up will ya!?\\nHowever, considering that in the next installments she will be much more likeable and kind of interesting, I\\'d say give it a shot and see if you like it: if you hate this first game, you might like the next, or can always stop here.\\nI recommend it.', 'genreID': 3, 'date': '2014-02-07'}\n"
     ]
    }
   ],
   "source": [
    "print(data[0])\n",
    "# Considering only the first 10000 samples\n",
    "dataset = data[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "punctuation = set(string.punctuation)\n",
    "for d in dataset:\n",
    "    r = ''.join([c for c in d['text'].lower() if c not in punctuation])\n",
    "    corpus.append(r)\n",
    "#print(corpus[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stats related to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique unigrams are 29692\n"
     ]
    }
   ],
   "source": [
    "# Reading through the text after removing punctuation\n",
    "wordCount = defaultdict(int)\n",
    "for document in corpus:\n",
    "    for word in document.split():\n",
    "        wordCount[word] += 1\n",
    "print(\"Number of unique unigrams are {}\".format(len(wordCount.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 - Number of unique bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique bigrams amongst the reviews are 256618\n",
      " The 5 top bigrams are mentioned as follows - \n",
      "[(4441, 'this game'), (4249, 'the game'), (3359, 'of the'), (2020, 'if you'), (2017, 'in the')]\n"
     ]
    }
   ],
   "source": [
    "bigram_map = defaultdict(int)\n",
    "for document in corpus:\n",
    "    words = document.split()\n",
    "    for i in range(len(words) - 1):\n",
    "        first_word = words[i]\n",
    "        second_word = words[i+1]\n",
    "        key = first_word + ' ' + second_word\n",
    "        bigram_map[key] += 1\n",
    "\n",
    "wordCount = [(bigram_map[key], key) for key in bigram_map]\n",
    "wordCount.sort()\n",
    "wordCount.reverse()\n",
    "\n",
    "print(\"Number of unique bigrams amongst the reviews are {}\".format(len(bigram_map.keys())))\n",
    "\n",
    "print(\" The 5 top bigrams are mentioned as follows - \")\n",
    "print(wordCount[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 - Here, we are using the 1000 Most common Bigrams as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "topNGrams = wordCount[:1000]\n",
    "bigrams = [val[1] for val in topNGrams]\n",
    "bigramId = dict(zip(bigrams, range(len(bigrams))))\n",
    "bigramSet = set(bigrams)\n",
    "\n",
    "def feature(datum):\n",
    "    feat = ([0]*len(bigrams))\n",
    "    r = ''.join([c for c in datum['text'].lower() if not c in punctuation])\n",
    "    words = r.split()\n",
    "    for i in range(len(words) - 1):\n",
    "        bigram = words[i] + ' ' + words[i + 1]\n",
    "        if bigram in bigrams:\n",
    "            feat[bigramId[bigram]] += 1\n",
    "    feat.append(1)\n",
    "    return feat\n",
    "\n",
    "X = [feature(d) for d in dataset]\n",
    "y = [math.log2(d['hours'] + 1) for d in dataset]\n",
    "\n",
    "clf = linear_model.Ridge(1.0, fit_intercept=False) # MSE + 1.0 l2\n",
    "clf.fit(X, y)\n",
    "theta = clf.coef_\n",
    "predictions = clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set MSE = 4.393787247200031\n"
     ]
    }
   ],
   "source": [
    "def MSE(actual, predictions):\n",
    "    differences = [(x-y)**2 for (x, y) in zip(actual, predictions)]\n",
    "    return sum(differences)/len(differences)\n",
    "\n",
    "# Computing the mean squared error\n",
    "mse_train = MSE(y, predictions)\n",
    "print(\"Training set MSE = {}\".format(mse_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 - Here, we are using the 1000 most common unigrams and bigrams as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set MSE = 4.9110892467357665\n"
     ]
    }
   ],
   "source": [
    "def feature(datum, K):\n",
    "    \"\"\" args : datum - data sample\n",
    "        K : Top K Grams to be filtered \"\"\"\n",
    "    r = ''.join([c for c in datum['text'].lower() if c not in punctuation])\n",
    "    words = r.split()\n",
    "    if(len(words) == 0):\n",
    "        feat = [0] * K\n",
    "        feat.append(1)\n",
    "        return feat\n",
    "    bigram_map = defaultdict(int)\n",
    "    unigram_map = defaultdict(int)\n",
    "    \n",
    "    # Bigram map is used to store bigram frequencies\n",
    "    # Unigram map is used to store unigram frequencies\n",
    "    for i in range(len(words) - 1):\n",
    "        bigram_key = words[i] + ' ' + words[i+1]\n",
    "        bigram_map[bigram_key] += 1\n",
    "        unigram_map[words[i]] += 1\n",
    "    #last_word = words[-1]\n",
    "    unigram_map[words[-1]] += 1\n",
    "    \n",
    "    # Compute the same for unigram and bigrams\n",
    "    wordCount_unigram = [(unigram_map[key], key) for key in unigram_map]\n",
    "    wordCount_bigram = [(bigram_map[key], key) for key in bigram_map]\n",
    "    \n",
    "    # Compute the wordCount for unigram and bigram\n",
    "    wordCount_unigram.sort()\n",
    "    wordCount_bigram.sort()\n",
    "    wordCount_unigram.reverse()\n",
    "    wordCount_bigram.reverse()\n",
    "    \n",
    "    # List of keys for the unigram and bigram\n",
    "    unigramKeys = [x[1] for x in wordCount_unigram]\n",
    "    bigramKeys = [x[1] for x in wordCount_bigram]\n",
    "    unigramId = dict(zip(unigramKeys, range(len(unigramKeys))))\n",
    "    bigramId = dict(zip(bigramKeys, range(len(bigramKeys))))\n",
    "    \n",
    "    # Feature of a combination of unigram and bigrams\n",
    "    topNGrams = []\n",
    "    \n",
    "    # Problem similar to merging two sorted lists with a condition on the max number of lists\n",
    "    i, i_u, i_b = 0, 0, 0\n",
    "    while (i < K and (i_u < len(wordCount_unigram) and i_b < len(wordCount_bigram))):\n",
    "        if(wordCount_unigram[i_u][0] > wordCount_bigram[i_b][0]):\n",
    "            topNGrams.append(wordCount_unigram[i_u][1])\n",
    "            i_u += 1\n",
    "        else:\n",
    "            topNGrams.append(wordCount_bigram[i_b][1])\n",
    "            i_b += 1\n",
    "        i += 1\n",
    "\n",
    "    while (i < K and (i_u < len(wordCount_unigram))):\n",
    "        topNGrams.append(wordCount_unigram[i_u][1])\n",
    "        i_u += 1\n",
    "        i += 1\n",
    "\n",
    "    while (i < K and (i_b < len(wordCount_bigram))):\n",
    "        topNGrams.append(wordCount_bigram[i_b][1])\n",
    "        i_b += 1\n",
    "        i += 1\n",
    "        \n",
    "    feat = ([0] * K)\n",
    "    nGrams = [val for val in topNGrams]\n",
    "    nGramId = dict(zip(nGrams, range(len(nGrams))))\n",
    "    nGramSet = set(nGramId)\n",
    "    for nGram in topNGrams:\n",
    "        # Logic to check whether it's a bigram or unigram?\n",
    "        if(len(nGram.split()) > 1):\n",
    "            feat[nGramId[nGram]] += 1\n",
    "        else:\n",
    "            assert(len(nGram.split()) == 1)\n",
    "            feat[nGramId[nGram]] += 1\n",
    "    feat.append(1)\n",
    "    return feat\n",
    "\n",
    "# Compute the feature matrix \"X\" for the given dataset\n",
    "X = [feature(d, 1000) for d in dataset]\n",
    "# Process the dataset for the matrix \"Y\"\n",
    "y = [math.log2(d['hours'] + 1) for d in dataset]\n",
    "# Compute the Linear Regression model here\n",
    "clf = linear_model.Ridge(1.0, fit_intercept=True) # MSE + 1.0 l2\n",
    "clf.fit(X, y)\n",
    "theta = clf.coef_\n",
    "predictions = clf.predict(X)\n",
    "\n",
    "mse_train = MSE(y, predictions)\n",
    "print(\"Training set MSE = {}\".format(mse_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 - Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First five IDFs are [-4.342727686267685e-05, -4.342727686267685e-05, -4.342727686267685e-05, -4.342727686267685e-05, -4.342727686267685e-05]\n",
      "First five TFs are [[0.004694835680751174, 0.004694835680751174, 0.004694835680751174, 0.004694835680751174, 0.004694835680751174], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0.007042253521126761], [0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "def computeIDF(corpus, searches):\n",
    "    idfs = []\n",
    "    numDocs = len(corpus)\n",
    "    for searchWord in searches:\n",
    "        count = 0\n",
    "        for document in corpus:\n",
    "            words = searchWord.split()\n",
    "            if(searchWord in words):\n",
    "                count += 1\n",
    "        idfs.append(math.log10(numDocs/(1 + count)))\n",
    "    return idfs\n",
    "\n",
    "def computeTF(corpus, searches):\n",
    "    (rows, cols) = (len(corpus), len(searches))\n",
    "    tfs = [[0 for i in range(cols)] for j in range(rows)]\n",
    "    for searchWord in searches:\n",
    "        for document in corpus:\n",
    "            words = document.split()\n",
    "            tf, numWords = 0, len(words)\n",
    "            word_idx = searches.index(searchWord)\n",
    "            doc_idx = corpus.index(document)\n",
    "            if(searchWord in words):\n",
    "                tfs[doc_idx][word_idx] += 1/numWords\n",
    "    \n",
    "    return tfs\n",
    "\n",
    "# Go through the entire list of searches to compute the IDF\n",
    "searches = [\"destiny\", \"annoying\", \"likeable\", \"chapter\", \"interesting\"]\n",
    "idf = computeIDF(corpus, searches)\n",
    "print(\"First five IDFs are {}\".format(idf[:5]))\n",
    "tf = computeTF(corpus, searches)\n",
    "print(\"First five TFs are {}\".format(tf[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF IDF scores for the reviewID r75487422"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word = destiny, TF score = 0.004694835680751174, IDF score = -4.342727686267685e-05, TFIDF score = -2.0388392893275517e-07\n",
      "Word = annoying, TF score = 0.004694835680751174, IDF score = -4.342727686267685e-05, TFIDF score = -2.0388392893275517e-07\n",
      "Word = likeable, TF score = 0.004694835680751174, IDF score = -4.342727686267685e-05, TFIDF score = -2.0388392893275517e-07\n",
      "Word = chapter, TF score = 0.004694835680751174, IDF score = -4.342727686267685e-05, TFIDF score = -2.0388392893275517e-07\n",
      "Word = interesting, TF score = 0.004694835680751174, IDF score = -4.342727686267685e-05, TFIDF score = -2.0388392893275517e-07\n"
     ]
    }
   ],
   "source": [
    "# review provides the entity in corpus\n",
    "review = corpus[0]\n",
    "tf, tf_idf = [0]*len(searches), [0] * len(searches)\n",
    "for i in range(len(searches)):\n",
    "    words = review.split()\n",
    "    numWords = len(words)\n",
    "    # Number of words = length of words in a document\n",
    "    count = 0\n",
    "    if (searches[i] in words):\n",
    "        count += 1\n",
    "    tf[i] = count / numWords\n",
    "    val = 1 if (tf[i] > 0) else 0\n",
    "    tf_idf[i] = tf[i] * idf[i]\n",
    "    print(\"Word = {}, TF score = {}, IDF score = {}, TFIDF score = {}\".format(searches[i], tf[i], idf[i], tf_idf[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adapt the unigram model to use the TF-IDF scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTopNUnigrams(corpus, N):\n",
    "    topNUnigrams = []\n",
    "    wordCount = defaultdict(int)\n",
    "    for document in corpus:\n",
    "        for word in document.split():\n",
    "            wordCount[word] += 1\n",
    "    \n",
    "    mostCommon = [(wordCount[x], x) for x in wordCount]\n",
    "    # Sort and reverse the code\n",
    "    mostCommon.sort()\n",
    "    mostCommon.reverse()\n",
    "    for i in range(N):\n",
    "        topNUnigrams.append(mostCommon[i][1])\n",
    "    return topNUnigrams\n",
    "\n",
    "def computeTFIDF(tf, idf):\n",
    "    len_tf = len(tf)\n",
    "    tf_idfs = []\n",
    "    for i in range(len_tf):\n",
    "        tf_idf = []\n",
    "        for j in range(len(tf[i])):\n",
    "            tf_idf.append(float(tf[i][j] * idf[j]))\n",
    "        tf_idfs.append(tf_idf)\n",
    "    return tf_idfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed features for the training set\n",
      "10000 1001\n",
      "Training set MSE = 5.242479008006355\n"
     ]
    }
   ],
   "source": [
    "def feature(corpus, N):\n",
    "    \"\"\" feature() is used to compute the features \"\"\"\n",
    "    topNUnigrams = computeTopNUnigrams(corpus, N)\n",
    "    #print(top1000Unigrams[:5])\n",
    "    idf = computeIDF(corpus, topNUnigrams)\n",
    "    tf = computeTF(corpus, topNUnigrams)\n",
    "    tf_idf = computeTFIDF(tf, idf)\n",
    "    for i in range(len(tf_idf)):\n",
    "        # Offset addition\n",
    "        tf_idf[i].append(1)\n",
    "    return tf_idf\n",
    "\n",
    "X_train = feature(corpus, 1000)\n",
    "Y_train = [math.log2(d['hours'] + 1) for d in dataset]\n",
    "print(\"Computed features for the training set\")\n",
    "print(len(X_train), len(X_train[0]))\n",
    "clf = linear_model.Ridge(1.0, fit_intercept=False) # MSE + 1.0 l2\n",
    "clf.fit(X_train, Y_train)\n",
    "theta = clf.coef_\n",
    "predictions = clf.predict(X_train)\n",
    "mse_train = MSE(Y_train, predictions)\n",
    "print(\"Training set MSE = {}\".format(mse_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The index of review r75487422 is 8328\n",
      "Most similar reviewID to r75487422 is r26288269 and is present at index 420\n"
     ]
    }
   ],
   "source": [
    "def norm(A):\n",
    "    val = 0\n",
    "    for ele in A:\n",
    "        val += ele**2\n",
    "    return val\n",
    "\n",
    "# Here, we have the cosine similarity being defined\n",
    "def cosine_similarity(A, B):\n",
    "    numerator = np.dot(A, B)\n",
    "    denominator = math.sqrt(norm(A) * norm(B))\n",
    "    return numerator/denominator\n",
    "\n",
    "# For every datapoint in the given dataset\n",
    "r_idx = -1\n",
    "N = 1000 # Number of unigrams to process\n",
    "X = feature(corpus, 1000)\n",
    "for d in dataset:\n",
    "    if(d['reviewID'] == \"r19740876\"):\n",
    "        r_idx = dataset.index(d)\n",
    "        break\n",
    "print(\"The index of review r75487422 is {}\".format(r_idx))\n",
    "max_sim = -1\n",
    "mostSimilarReviewId = -1\n",
    "max_idx = -1\n",
    "for i in range(len(dataset)):\n",
    "    if(i != r_idx):\n",
    "        cos_sim = cosine_similarity(X[r_idx], X[i])\n",
    "        if(cos_sim > max_sim):\n",
    "            mostSimilarReviewId = dataset[i]['reviewID']\n",
    "            max_sim = cos_sim\n",
    "            max_idx = i\n",
    "\n",
    "print(\"Most similar reviewID to r75487422 is {} and is present at index {}\".format(mostSimilarReviewId, max_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'userID': 'u46449878', 'genre': 'RPG', 'early_access': False, 'reviewID': 'r19740876', 'hours': 0.6, 'text': 'NOTE: This game is Free To Play, meaning this review and many others do not count. I cannot stress enough that you play it for yourself with an open mind to get an undilluted opinion and ignore this review and any others.\\nSo... I know what Depression is. I\\'ve been diagnosed with it for the last 15 years alongside High Anxiety for all that time.\\nYou want to know what Depression is like? Let me tell you instead of you playing this game.\\nImagine sadness. Imagine extreme self-disappointment. Imagine self-loathing to as high of a degree as it\\'ll go. It\\'s not always the same, but to my knowledge... Mix them into one emotion and you get Depression.\\nIt is literally the worst emotion on the planet. Mostly all suicides are caused by feelings of Depression. I have no study, no way to prove that, other than just plain logic and understanding.\\nThis game does not convey the feelings of Depression.\\nIt is sad, yes, but Sadness is not Depression. On it\\'s own that is. You need a slew of other different, but similar emotions but also the faint hope that it can get better and mental habits of being pessimistic about your life, you and everything around you.\\nI\\'ve played and read all lines of both endings and choice options. there\\'s multiple choices in most cases, but in most cases the options are locked because the dev most likely didn\\'t program that far in a text-based game. I could also say because of [arbitrary/contrived reason].\\nLet me be perfectly clear. This game is not depressing, it\\'s not saddening. To me... It\\'s insulting how hyped this game is on the store page.\\nIt\\'s got the pacing of a pop-up book, the writing of an overly-emotional stereotype writing about how awful life is and for a game that boasts \"multiple endings\" I would have imagined that the developer would\\'ve coded in a suicide ending, since I literally did every awful option and nothing really different happened.\\nThe game is as engaging as listenning to someone whine about how awful their life is. I know, it\\'s supposed to convey the feelings of being Depressed, but the thing is... Unless you\\'re a wordsmith (which The Quinnspiracy isn\\'t) then it will never work in text form.\\nThe game would have worked better as a more interactive game that isn\\'t text-based or an auto-biography of part of the dev\\'s life.\\nWhile I was playing-no... While I was reading this e-book it felt like I was reading what amounted to angsty, emo fanfiction and not even good fanfiction.\\nSome options of the game aren\\'t labeled too great, sometimes the result being the opposite of what you thought the option would do (so... control problems in a text-based game?) which basically boils down to those options being improperly labeled.\\nThe soundtrack is... well it\\'s actually just a track, one song looping over and over and not even with a clean loop either, at certain times you\\'ll hear crackling or popping in the audio as it loops.\\nAll in all, this game utterly fails at what it tries to do. Not only that, but it makes Depression laughable, comparable to stupid pre-teens who thinks depression is a cool thing, who we all know are a complete joke, and that\\'s the literal opposite of Depression.\\nI\\'ve seen better pacing in books you can read in any library anywhere, the game wasn\\'t at all uncomfortable for me, it wasn\\'t like an unfiltered mental diary, more like a \"pity me, I\\'m sad right?\" type of story written by some spoiled kid who got grounded for the first time.\\nJust like any poorly-made text-based game, you can look at the first sentence/paragraph, the last one aswell and essentially miss nothing story-wise, ignoring everything inbetween, only with this one, you can look at jsut the last sentence or two and know all you need for the next decision.\\nThe Choice-based content is actually true to a degree, but the multiple endings are comparable to Mass Effect 3\\'s \"What color will your ship\\'s explosion be\" type of ending, each one summing up all you\\'ve done, with ONE sentence of difference ignoring all else (summing up your run doesn\\'t really count) which doesn\\'t matter because this game is extremely far from being long, even if you read it all. I stopped reading all but the last sentence and first one halfway through.\\nPros:\\n- Content based on Choice is true to a degree.\\n- The \"art style\" of the game works decently.\\nCons:\\n- Completely makes a joke of Depression.\\n- Fails to give an understanding view of depression to the player.\\n- Text drones on about the player\\'s awful life and at times feels like empty complaining and whining.\\n- There isn\\'t really music, just ambience with awful loop crackles. It loops 3 or 4 times in a 15 minute period.\\n- You can ignore most of the text without loss of story/choice context.\\n- Some choices are literally always locked no matter what you do.\\n- There\\'s a choice or two on the first screen that is locked.\\n- Due to the third point, the game is dull and couldn\\'t be further from being engaging even if it\\'s in 1st person.\\n- The wording in some pages are poorly done, or done wierdly to confuse the reader.\\n- The Endings are technically the same save for one or two lines of text that don\\'t change anything.\\nThis game gets a 1/10. It makes a joke out of Depression by coming off as whiny and complaining about how awful life is, saying how awful everything is rather than using words to imply the feelings so the reader can dig and feel them that way.\\nIf that wasn\\'t a thing and it did what it\\'d do, it would be a 5/10 simply due to everything else just being passable. It\\'s not well-made GameSpy, it\\'s just passable. I\\'ve played many text-based games that are more well executed than this, some of them being Zork, Trials in Tainted Space and Unification Wars. Trials is in beta.\\nTo improve it I would suggest a complete redo of the whole game. It needs or be worded better, don\\'t tell us, make us imagine it so we can be THERE, not watching ourselves like a ghost would. Be less preachy or Emo-y (if that\\'s a word).\\nI would also suggest the ambience loop be cleaned up and have the crackles removed.\\nMake the endings ALOT more dramatically different. Not a single sentence of difference, I\\'m talking the difference between a life that will continue to be happy, or literally suicide (say if you choose all the good options and then all the bad ones in another playthrough).\\nIt doesn\\'t even need to be longer.\\nWill I give it Forbidden status? Well... based on how it\\'s been released a full year, I\\'m inclined to say yes, but I will hold off on that as per the usual for my statuses.\\nI live with depression every day. I know how it feels. This game? Not one single moment did I EVER empathize with the character. It fails at it\\'s intended purpose.', 'genreID': 2, 'date': '2015-08-22'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[8328])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTopNBigrams(corpus, N):\n",
    "    topNBigrams = []\n",
    "    wordCount = defaultdict(int)\n",
    "    for document in corpus:\n",
    "        words = document.split()\n",
    "        for i in range(len(words)-1):\n",
    "            key = word[i] + ' ' + word[i+1]\n",
    "            wordCount[key] += 1\n",
    "    \n",
    "    mostCommon = [(wordCount[x], x) for x in wordCount]\n",
    "    # Sort and reverse the code\n",
    "    mostCommon.sort()\n",
    "    mostCommon.reverse()\n",
    "    \n",
    "    for i in range(N):\n",
    "        topNBigrams.append(mostCommon[i][1])\n",
    "    return topNBigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\\\', ';', '$', '`', '/', '!', '%', ']', ':', '|', '-', '+', '\"', '=', '{', '^', '~', '*', \"'\", ',', '_', ')', '#', '>', '(', '&', '@', '}', '[', '.', '?', '<'}\n"
     ]
    }
   ],
   "source": [
    "print(set(string.punctuation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison between unigram and bigram models for different variants of regularization parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unigram + Removed Punctuation + TD IDF Scores = 0, 0, 0\n",
    "\n",
    "Unigram + Removed Punctuation + Word Counts = 0, 0, 1\n",
    "\n",
    "Unigram + Preserve Punctuation + TD IDF Scores = 0, 1, 0\n",
    "\n",
    "Unigram + Preserve Punctuation + Word Counts = 0, 1, 1\n",
    "\n",
    "Bigram  + Removed Punctuation + TD IDF Scores = 1, 0, 0\n",
    "\n",
    "Bigram + Removed Punctuation + Word Counts = 1, 0, 1\n",
    "\n",
    "Bigram + Preserve Punctuation + TD IDF Scores = 1, 1, 0\n",
    "\n",
    "Bigram + Preserve Punctuation + Word Counts = 1, 1, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def computeCount(wordCorpus, gram_type, N):\n",
    "    \"\"\" args - wordCorpus - 2D list (each row - list of words in one review)\n",
    "                gram_type - Unigram (or) Bigram\n",
    "                N - Number of grams to search for \"\"\"\n",
    "    features = []\n",
    "    for words in wordCorpus:\n",
    "        length = len(words) if (gram_type == \"unigram\") else len(words) - 1\n",
    "        wordCount = defaultdict(float)\n",
    "        for i in range(length):\n",
    "            if(gram_type == \"unigram\"):\n",
    "                wordCount[words[i]] += 1\n",
    "            else:\n",
    "                wordCount[words[i] + ' ' + words[i+1]] += 1\n",
    "        mostCommon = [(wordCount[x], x) for x in wordCount]\n",
    "        mostCommon.sort()\n",
    "        mostCommon.reverse()\n",
    "        topNGrams = [] # store for every document in the corpus\n",
    "        for i in range(N):\n",
    "            if(i < len(mostCommon)):\n",
    "                topNGrams.append(mostCommon[i][1])\n",
    "        #print(\"Length of topNGrams = {}\".format(len(topNGrams)))\n",
    "        # Compute the unigramId and accordingly prepare the feature vector\n",
    "        ngramId = dict(zip(topNGrams, range(len(topNGrams))))\n",
    "        feature = [0] * N\n",
    "        for word in words:\n",
    "            if(word in topNGrams):\n",
    "                feature[ngramId[word]] += 1\n",
    "        feature.append(1) # For the constant \\theta_0\n",
    "        features.append(feature)\n",
    "        \n",
    "    return features\n",
    "\n",
    "def computeTFIDF(wordCorpus, gram_type, N):\n",
    "    \"\"\" args - wordCorpus - 2D list (each row - list of words in one review)\n",
    "                gram_type - Unigram (or) Bigram\n",
    "                N - Number of grams to search for \"\"\"\n",
    "    features, tfs, idfs = [], [], []\n",
    "    numDocuments = defaultdict(int)\n",
    "    for words in wordCorpus:\n",
    "        length = len(words) if (gram_type == \"unigram\") else len(words) - 1\n",
    "        for i in range(length):\n",
    "            if(gram_type == \"unigram\"):\n",
    "                numDocuments[words[i]] += 1\n",
    "            else:\n",
    "                numDocuments[words[i] + ' ' + words[i+1]] += 1\n",
    "    \n",
    "    for words in wordCorpus:\n",
    "        # Logic to compute most frequent unigrams or bigrams\n",
    "        length = len(words) if (gram_type == \"unigram\") else len(words) - 1\n",
    "        wordCount = defaultdict(float)\n",
    "        for i in range(length):\n",
    "            if(gram_type == \"unigram\"):\n",
    "                wordCount[words[i]] += 1\n",
    "                numDocuments[words[i]] += 1\n",
    "            else:\n",
    "                wordCount[words[i] + ' ' + words[i+1]] += 1\n",
    "                numDocuments[words[i] + ' ' + words[i+1]] += 1\n",
    "        \n",
    "        mostCommon = [(wordCount[x], x) for x in wordCount]\n",
    "        mostCommon.sort()\n",
    "        mostCommon.reverse()\n",
    "        topNGrams = [] # store for every document in the corpus\n",
    "        for i in range(N):\n",
    "            if(i < len(mostCommon)):\n",
    "                topNGrams.append(mostCommon[i][1])\n",
    "        \n",
    "        ngramId = dict(zip(topNGrams, range(len(topNGrams))))\n",
    "        # Compute TF\n",
    "        tf = [0] * N\n",
    "        numWords = len(words)\n",
    "        for word in words:\n",
    "            if(word in ngramId):\n",
    "                tf[ngramId[word]] += 1/numWords\n",
    "        # Compute IDF\n",
    "        # Number of documents that have the term present in it\n",
    "        idf = [0] * N\n",
    "        for i in range(N):\n",
    "            if(i < len(topNGrams)):\n",
    "                idf[i] = math.log10(len(wordCorpus)/(numDocuments[topNGrams[i]] + 1))\n",
    "        feature = [tf[i] * idf[i] for i in range(N)]\n",
    "        # For constant \\theta_0\n",
    "        feature.append(1)\n",
    "        features.append(feature)\n",
    "\n",
    "    return features\n",
    "\n",
    "# Customized implementation of split function\n",
    "def customSplit(characters, delimiters):\n",
    "    result = []\n",
    "    word = \"\"\n",
    "    for char in characters:\n",
    "        if(char in delimiters):\n",
    "            if(len(word) > 0):\n",
    "                result.append(word)\n",
    "            if(char != ' '):\n",
    "                word = str(char)\n",
    "                result.append(word)\n",
    "            word = \"\"\n",
    "        else:\n",
    "            word += char\n",
    "    return result\n",
    "\n",
    "def computeFeatures(dataset, gram_type, punct_type, score_type, N):\n",
    "    \"\"\" dataset - Input data for which the feature needs to be extracted\n",
    "        gram_type - Unigram or Bigram\n",
    "        punct_type - Preserve or remove punctuations\n",
    "        score_type - TF-IDF or wordCount \"\"\"\n",
    "    features, wordCorpus, corpus = [], [], []\n",
    "    for d in dataset:\n",
    "        if(punct_type == \"remove\"):\n",
    "            review = ''.join([c for c in d['text'].lower() if c not in punctuation])\n",
    "        elif(punct_type == \"preserve\"):\n",
    "            review = ''.join([c for c in d['text'].lower()])\n",
    "        corpus.append(review)\n",
    "    punctList = [ch for ch in string.punctuation]\n",
    "    # For every document in the list of all documents\n",
    "    for document in corpus:\n",
    "        delimiters = [' ']\n",
    "        if(punct_type == \"preserve\"):\n",
    "            for delimit in string.punctuation:\n",
    "                delimiters.append(delimit)\n",
    "            words = customSplit(document, delimiters)\n",
    "        else:\n",
    "            assert(punct_type == \"remove\")\n",
    "            words = customSplit(document, delimiters)\n",
    "        wordCorpus.append(words)\n",
    "        \n",
    "    if(score_type == \"wordCount\"):\n",
    "        features = computeCount(wordCorpus, gram_type, N)\n",
    "    else:\n",
    "        assert(score_type == \"tf_idf\")\n",
    "        features = computeTFIDF(wordCorpus, gram_type, N)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MSE with key = unigram|preserve|tf_idf|0.01 is 5.357231999710872\n",
      "Validation MSE with key = unigram|preserve|tf_idf|0.1 is 5.332903487594727\n",
      "Validation MSE with key = unigram|preserve|tf_idf|1.0 is 5.321886691089576\n",
      "Validation MSE with key = unigram|preserve|tf_idf|10.0 is 5.320347749905078\n",
      "Validation MSE with key = unigram|preserve|tf_idf|100.0 is 5.317046585844409\n",
      "Test MSE at unigram, preserve, tf_idf and lambda = 100.0 is 5.198895607621101\n",
      "Validation MSE with key = unigram|preserve|wordCount|0.01 is 5.514462641035678\n",
      "Validation MSE with key = unigram|preserve|wordCount|0.1 is 5.489822087879586\n",
      "Validation MSE with key = unigram|preserve|wordCount|1.0 is 5.417046530978679\n",
      "Validation MSE with key = unigram|preserve|wordCount|10.0 is 5.3487883610962035\n",
      "Validation MSE with key = unigram|preserve|wordCount|100.0 is 5.309129178745707\n",
      "Test MSE at unigram, preserve, wordCount and lambda = 100.0 is 5.206755660610267\n",
      "Validation MSE with key = unigram|remove|tf_idf|0.01 is 5.333881336668315\n",
      "Validation MSE with key = unigram|remove|tf_idf|0.1 is 5.315217056334459\n",
      "Validation MSE with key = unigram|remove|tf_idf|1.0 is 5.31048010480832\n",
      "Validation MSE with key = unigram|remove|tf_idf|10.0 is 5.315102638667783\n",
      "Validation MSE with key = unigram|remove|tf_idf|100.0 is 5.317233944627375\n",
      "Test MSE at unigram, remove, tf_idf and lambda = 1.0 is 5.210768486373123\n",
      "Validation MSE with key = unigram|remove|wordCount|0.01 is 5.454944679317683\n",
      "Validation MSE with key = unigram|remove|wordCount|0.1 is 5.435021036146686\n",
      "Validation MSE with key = unigram|remove|wordCount|1.0 is 5.381874790634799\n",
      "Validation MSE with key = unigram|remove|wordCount|10.0 is 5.3386169774959376\n",
      "Validation MSE with key = unigram|remove|wordCount|100.0 is 5.308760465041253\n",
      "Test MSE at unigram, remove, wordCount and lambda = 100.0 is 5.2810556682052665\n",
      "Validation MSE with key = bigram|preserve|tf_idf|0.01 is 5.316094612279973\n",
      "Validation MSE with key = bigram|preserve|tf_idf|0.1 is 5.316094612279973\n",
      "Validation MSE with key = bigram|preserve|tf_idf|1.0 is 5.316094612279973\n",
      "Validation MSE with key = bigram|preserve|tf_idf|10.0 is 5.316094612279973\n",
      "Validation MSE with key = bigram|preserve|tf_idf|100.0 is 5.316094612279973\n",
      "Test MSE at bigram, preserve, tf_idf and lambda = 0.01 is 5.199761277161803\n",
      "Validation MSE with key = bigram|preserve|wordCount|0.01 is 5.316094612279973\n",
      "Validation MSE with key = bigram|preserve|wordCount|0.1 is 5.316094612279973\n",
      "Validation MSE with key = bigram|preserve|wordCount|1.0 is 5.316094612279973\n",
      "Validation MSE with key = bigram|preserve|wordCount|10.0 is 5.316094612279973\n",
      "Validation MSE with key = bigram|preserve|wordCount|100.0 is 5.316094612279973\n",
      "Test MSE at bigram, preserve, wordCount and lambda = 0.01 is 5.199761277161803\n",
      "Validation MSE with key = bigram|remove|tf_idf|0.01 is 5.316094612279973\n",
      "Validation MSE with key = bigram|remove|tf_idf|0.1 is 5.316094612279973\n",
      "Validation MSE with key = bigram|remove|tf_idf|1.0 is 5.316094612279973\n",
      "Validation MSE with key = bigram|remove|tf_idf|10.0 is 5.316094612279973\n",
      "Validation MSE with key = bigram|remove|tf_idf|100.0 is 5.316094612279973\n",
      "Test MSE at bigram, remove, tf_idf and lambda = 0.01 is 5.199761277161803\n",
      "Validation MSE with key = bigram|remove|wordCount|0.01 is 5.316094612279973\n",
      "Validation MSE with key = bigram|remove|wordCount|0.1 is 5.316094612279973\n",
      "Validation MSE with key = bigram|remove|wordCount|1.0 is 5.316094612279973\n",
      "Validation MSE with key = bigram|remove|wordCount|10.0 is 5.316094612279973\n",
      "Validation MSE with key = bigram|remove|wordCount|100.0 is 5.316094612279973\n",
      "Test MSE at bigram, remove, wordCount and lambda = 0.01 is 5.199761277161803\n"
     ]
    }
   ],
   "source": [
    "def validation_pipeline(data):\n",
    "    random.seed(0)\n",
    "    random.shuffle(data)\n",
    "    # size = 10000\n",
    "    training_set = data[:10000]\n",
    "    validation_set = data[10000:20000]\n",
    "    test_set = data[20000:30000]\n",
    "    # Regularization parameters\n",
    "    lambdas = [10 ** i for i in np.arange(-2,3,dtype=float)]\n",
    "    # Different options for the loop\n",
    "    gramTypes = [\"unigram\", \"bigram\"]\n",
    "    punctuationTypes = [\"preserve\", \"remove\"]\n",
    "    countTypes = [\"tf_idf\", \"wordCount\"]\n",
    "    # Place to store MSE values for all the models\n",
    "    mseVals = defaultdict(float)\n",
    "    bestModels = defaultdict(float)\n",
    "    # For each gramType in the list of gramTypes\n",
    "    for gramType in gramTypes:\n",
    "        for puncType in punctuationTypes:\n",
    "            for countType in countTypes:\n",
    "                # Training set is as follows\n",
    "                X_train = computeFeatures(training_set, gramType, puncType, countType, 1000)\n",
    "                Y_train = [math.log2(d['hours'] + 1) for d in training_set]\n",
    "                # Validation set is as follows\n",
    "                X_val = computeFeatures(validation_set, gramType, puncType, countType, 1000)\n",
    "                Y_val = [math.log2(d['hours'] + 1) for d in validation_set]\n",
    "                # Test set is as follows\n",
    "                X_test = computeFeatures(test_set, gramType, puncType, countType, 1000)\n",
    "                Y_test = [math.log2(d['hours'] + 1) for d in test_set]\n",
    "                # Ensure that the length of the training set \"equals\" 1001\n",
    "                assert(len(X_train[0]) == 1001)\n",
    "                minMSE, minlamb = 10000, -1\n",
    "                for lamb in lambdas:\n",
    "                    clf = linear_model.Ridge(lamb, fit_intercept=True)\n",
    "                    clf.fit(X_train, Y_train)\n",
    "                    #theta = clf.coef_\n",
    "                    Y_val_pred = clf.predict(X_val)\n",
    "                    mse_val = MSE(Y_val, Y_val_pred)\n",
    "                    key = str(gramType) + '|' + str(puncType) + '|' + str(countType) + '|' + str(lamb)\n",
    "                    print(\"Validation MSE with key = {} is {}\".format(key, mse_val))\n",
    "                    mseVals[key] = mse_val\n",
    "                    if(mse_val < minMSE):\n",
    "                        minMSE = mse_val\n",
    "                        minlamb = lamb\n",
    "                k = str(gramType) + '|' + str(puncType) + '|' + str(countType) + '|' + str(minlamb)\n",
    "                bestModels[k] = minMSE\n",
    "                clf = linear_model.Ridge(minlamb, fit_intercept=True)\n",
    "                clf.fit(X_train, Y_train)\n",
    "                Y_test_pred = clf.predict(X_test)\n",
    "                mse = MSE(Y_test, Y_test_pred)\n",
    "                print(\"Test MSE at {}, {}, {} and lambda = {} is {}\".\\\n",
    "                      format(gramType, puncType, countType, minlamb, mse))\n",
    "                \n",
    "validation_pipeline(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unigram model with preserve punctuations and TF IDF score performs better than all models on the test set. The above tables have the values for validation set MSE and test set MSE for a total of 48 configurations (40 for validation set and 8 for test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

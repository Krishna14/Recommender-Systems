{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle Team - Severus Snape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task : Play Prediction\n",
    "\n",
    "Different approaches experimented -\n",
    "\n",
    "1. Popularity based metrics - Baseline\n",
    "2. Jaccard similarity - Performs worse than strong baseline\n",
    "3. Cosine similarity - Performs worse than strong baseline\n",
    "4. Popularity + similarity based model - Strong baseline\n",
    "5. Features extracted (similarity, popularity) + Training a classifier\n",
    "\n",
    "   5.1 Features extracted = (similarity between users, popularity of the given game)\n",
    "   \n",
    "   5.2 Classifiers used - \n",
    "       5.2.1 (Logistic regression (Value of C was found to be $10^{-7}$ for the validation set accuracy = 0.69)\n",
    "       5.2.2 (AdaBoost - To imitate ensemble learning)\n",
    "       \n",
    "       - 5 fold cross validation was used for computing the hyperparameters for the above mentioned classifiers.\n",
    "       The model was later discarded due to a loss in performance.\n",
    "       \n",
    "       - The accuracy of classifiers was lower by 1.6% on the validation set and thus, the original strong baseline model was retained.\n",
    "       \n",
    "\n",
    "Methodology and Evaluation -\n",
    "\n",
    "For all approaches that involve classification, data duplication was performed for the entire dataset to produce negative samples on the dataset for the classifier to be trained. \n",
    "\n",
    "1. Accuracy was not the only metric used for evaluation. Just like in the homeworks, Balanced Error Rate (BER)\n",
    "   was also used for evaluation purposes.\n",
    "2. In this scenario, both the BER and accuracy were the best for a similarity threshold of 0.04 and popularity        threshold = 80 (number of users who played the same game as the query game)\n",
    "3. There's a chance of overfit with this model as these are customized values for this dataset. There's a good        chance that this model doesn't perform well on another dataset. So, I used a classifier to train the                features obtained from this similarity, popularity approach. It didn't give better performance and thus            discarded.\n",
    "\n",
    "The model that's finally submitted is the one that's providing better performance on leaderboard. The strong baseline model with thresholds empirically computed on the validation and test sets. The biggest learning is the principle of Occam's razor (simplest explanation is often the better performing one)\n",
    "\n",
    "The code for the same has been provided below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task : Time played prediction\n",
    "\n",
    "Different approaches -\n",
    "\n",
    "1. Time played = $\\alpha + \\beta_u + \\beta_i$\n",
    "\n",
    "2. Time played = $\\alpha + \\beta_u + \\beta_i + \\gamma_u . \\gamma_i$\n",
    "\n",
    "Data preparation and pre-processing -\n",
    "\n",
    "- 80% and 20 % for this example for both the models (140000 training and 35000 validation samples)\n",
    "\n",
    "Methodology and evaluation - \n",
    "\n",
    "- Implemented a customized version of early stopping for the complete latent factor model to avoid overfitting\n",
    "- Used 5 fold cross validation for estimation of hyperparameters.\n",
    "- Grid search was used (and was very runtime consuming) for deciding on the hyper-parameters for the model.\n",
    "- Evaluated models based on MSE of validation set.\n",
    "\n",
    "Results and Conclusion - \n",
    "\n",
    "- Simulated a customized version of mini-batch gradient descent with 5 fold cross validation to prevent\n",
    "  overfitting of parameters for the training set.\n",
    "  - Stop SGD after 5 iterations and check whether the following condition is satisfied -\n",
    "    - (prev MSE - current MSE > 0) (prev MSE and current MSE are values on the validation set)\n",
    "    - Break out of the loop if it's not satisfied. This is very similar to the early stopping criterion that's commonly used to avoid overfitting.\n",
    "    - Choose the best model that results in the minimum value of MSE.\n",
    "    - For this problem, the following values were obtained for the complete latent factor model\n",
    "      - Number of latent factors = 3\n",
    "      - Value of regression coefficient ($\\lambda$) = 10 / 140000\n",
    "      - Overall MSE on test set = 3.0985\n",
    "  \n",
    "- Achieved a slightly worse performant model (MSE = 3.098) than the strong baseline set by the first model (3.078).\n",
    "\n",
    "- But, considering the fact that there are more variables whose values impact the performance of the model, I choose to proceed with the simpler model because it has fewer variables and the problem is a convex optimization problem and gradient descent provides the optimal solution to that problem given those values.\n",
    "\n",
    "- Also, the complete latent factor model has a lot of chances of variance (depending on the initialized values of $\\gamma_u$ and $\\gamma_i$, values of K, Gradient descent step size)\n",
    "\n",
    "- Similar to the above approach, I am using Occam's razor and continuing to use the simpler version of the latent factor model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
